# 一. 分布式基础

分布式系统的核心：可扩展性、不出现单点故障、服务或者存储无状态等特点。

## 1.1 CAP理论

CAP 理论可以表述为，一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容忍性（Partition Tolerance）这三项中的两项。

**一致性**是指“所有节点同时看到相同的数据”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有数据的最新版本。

**可用性**是指“任何时候，读写都是成功的”，即服务一直可用，而且是正常响应时间。我们平时会看到一些 IT 公司的对外宣传，比如系统稳定性已经做到 3 个 9、4 个 9，即 99.9%、99.99%，这里的 N 个 9 就是对可用性的一个描述，叫做 SLA，即服务水平协议。比如我们说月度 99.95% 的 SLA，则意味着每个月服务出现故障的时间只能占总时间的 0.05%，如果这个月是 30 天，那么就是 21.6 分钟。

**分区容忍性**具体是指“当部分节点出现消息丢失或者分区故障的时候，分布式系统仍然能够继续运行”，即系统容忍网络出现分区，并且在遇到某节点或网络分区之间网络不可达的情况下，仍然能够对外提供满足一致性和可用性的服务。

在分布式系统中，由于系统的各层拆分，P 是确定的，CAP 的应用模型就是 CP 架构和 AP 架构。分布式系统所关注的，就是在 Partition Tolerance 的前提下，如何实现更好的 A 和更稳定的 C。

C与A是处于一个动态平衡的过程中。

业务上对一致性的要求会直接反映在系统设计中，典型的就是 CP 和 AP 结构。

- **CP 架构**：对于 CP 来说，放弃可用性，追求一致性和分区容错性。

  > 在 CAP 模型中，ZooKeeper 是 CP，这意味着面对网络分区时，为了保持一致性，它是不可用的。

- **AP 架构**：对于 AP 来说，放弃强一致性，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的 Base 也是根据 AP 来扩展的。

  > Eureka 是 Spring Cloud 微服务技术栈中的服务发现组件，Eureka 的各个节点都是平等的，几个节点挂掉不影响正常节点的工作，剩余的节点依然可以提供注册和查询服务，只要有一台 Eureka 还在，就能保证注册服务可用，只不过查到的信息可能不是最新的版本，不保证一致性。

## 1.2 Base 理论

Base 是三个短语的简写，即基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）.

**基本可用**
基本可用比较好理解，就是不追求 CAP 中的「任何时候，读写都是成功的」，而是系统能够基本运行，一直提供服务。基本可用强调了分布式系统在出现不可预知故障的时候，允许损失部分可用性，相比正常的系统，可能是响应时间延长，或者是服务被降级。

**软状态**

原子性可以理解为一种“硬状态”，软状态则是允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。

**最终一致性**

数据不可能一直是软状态，必须在一个时间期限之后达到各个节点的一致性，在期限过后，应当保证所有副本保持数据一致性，也就是达到数据的最终一致性。



Base 理论是在 CAP 上发展的，CAP 理论描述了分布式系统中数据一致性、可用性、分区容错性之间的制约关系，当你选择了其中的两个时，就不得不对剩下的一个做一定程度的牺牲。

Base 理论则是对 CAP 理论的实际应用，也就是在分区和副本存在的前提下，通过一定的系统设计方案，放弃强一致性，实现基本可用，这是大部分分布式系统的选择，比如 NoSQL 系统、微服务架构。

## 1.3 Paxos 算法

Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是， 在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执 行一个“一致性算法”以保证每个节点看到的指令一致。zookeeper 使用的 zab 算法是该算法的一个实现。

在 Paxos 协议中，有三类节点角色，分别是 Proposer、Acceptor 和 Learner，另外还有一个 Client，作为产生议题者。

<img src="https://gitee.com/adambang/pic/raw/master/20210117204839.png" alt="image-20210117204839793" style="zoom:50%;" />

上述三类角色只是逻辑上的划分，在工作实践中，一个节点可以同时充当这三类角色。

- Proposer： 只要 Proposer 发的提案被半数以上 Acceptor 接受，Proposer 就认为该提案里的 value 被选定 了。
-  Acceptor： 只要 Acceptor 接受了某个提案，Acceptor 就认为该提案里的 value 被选定了。
-  Learner： Acceptor 告诉 Learner 哪个 value 被选定，Learner 就认为那个 value 被选定。

Paxos算法分为两个阶段。具体如下：

 阶段一（准leader确定 ）：

 (a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。

 (b) 如果一个 Acceptor 收到一个编号为N的 Prepare 请求，且N大于该 Acceptor 已经响应过的 所有 Prepare 请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响 应反馈给 Proposer，同时该 Acceptor 承诺不再接受任何编号小于N的提案。 

阶段二（leader确认）：

 (a) 如果 Proposer 收到半数以上 Acceptor 对其发出的编号为N 的 Prepare 请求的响应，那么它 就会发送一个针对[N,V]提案的 Accept 请求给半数以上的 Acceptor。注意：V 就是收到的响应中 编号最大的提案的 value，如果响应中不包含任何提案，那么V 就由 Proposer 自己决定。

 (b) 如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor 没有对编号 大于N的 Prepare 请求做出过响应，它就接受该提案。

## 1.4 ZAB算法

## 1.5 Raft协议

## 1.6 Gossip协议

# 二.分布式事务

顾名思义，分布式事务关注的是分布式场景下如何处理事务，是指事务的参与者、支持事务操作的服务器、存储等资源分别位于分布式系统的不同节点之上。

简单来说，分布式事务就是一个业务操作，是由多个细分操作完成的，而这些细分操作又分布在不同的服务器上；事务，就是这些操作要么全部成功执行，要么全部不执行。

在实际开发中，分布式事务产生的**原因**主要来源于存储和服务的拆分。

## 2.1 分布式事务解决方案

分布式事务的解决方案，典型的有两阶段和三阶段提交协议、 TCC 分段提交，和基于消息队列的最终一致性设计。



### 2.1.1 2PC 两阶段提交

> 二阶段和三阶段提交协议都是引入了一个协调者的组件来统一调度所有分布式节点的执行，让当前节点知道其他节点的任务执行状态，通过通知和表决的方式，决定执行 Commit 还是 Rollback 操作。

两阶段提交（2PC，Two-phase Commit Protocol）是非常经典的强一致性、中心化的原子提交协议，在各种事务和一致性的解决方案中，都能看到两阶段提交的应用。

两阶段提交中的两个阶段，指的是 Commit-request 阶段和 Commit 阶段，两阶段提交的流程如下：

![image-20210117205228869](https://gitee.com/adambang/pic/raw/master/20210117205229.png)

1. 提交请求阶段
   在提交请求阶段，协调者将通知事务参与者准备提交事务，然后进入表决过程。在表决过程中，参与者将告知协调者自己的决策：同意（事务参与者本地事务执行成功）或取消（本地事务执行故障），在第一阶段，参与节点并没有进行Commit操作。

2. 提交阶段
   在提交阶段，协调者将基于第一个阶段的投票结果进行决策：提交或取消这个事务。这个结果的处理和前面基于半数以上投票的一致性算法不同，必须当且仅当所有的参与者同意提交，协调者才会通知各个参与者提交事务，否则协调者将通知各个参与者取消事务。

**两阶段提交存在的问题**：

- 资源被同步阻塞

  在执行过程中，所有参与节点都是事务独占状态，当参与者占有公共资源时，那么第三方节点访问公共资源会被阻塞。

- 协调者可能出现单点故障

  一旦协调者发生故障，参与者会一直阻塞下去。

- 在 Commit 阶段出现数据不一致

  在第二阶段中，假设协调者发出了事务 Commit 的通知，但是由于网络问题该通知仅被一部分参与者所收到并执行 Commit，其余的参与者没有收到通知，一直处于阻塞状态，那么，这段时间就产生了数据的不一致性。

### 2.1.2 3PC 三阶段提交

为了解决二阶段协议中的同步阻塞等问题，三阶段提交协议在协调者和参与者中都引入了超时机制，并且把两阶段提交协议的第一个阶段拆分成了两步：询问，然后再锁资源，最后真正提交。

三阶段中的 Three Phase 分别为 CanCommit、PreCommit、DoCommit 阶段。

![image-20210117205257597](https://gitee.com/adambang/pic/raw/master/20210117205257.png)

1. CanCommit 阶段
   3PC 的 CanCommit 阶段其实和 2PC 的准备阶段很像。协调者向参与者发送 Can-Commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。

2. PreCommit 阶段
   协调者根据参与者的反应情况来决定是否可以继续事务的 PreCommit 操作。根据响应情况，有以下两种可能。

   A. 假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会进行事务的预执行：

   发送预提交请求，协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段；

   事务预提交，参与者接收到 PreCommit 请求后，会执行事务操作；

   响应反馈，如果参与者成功执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。

   B. 假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就中断事务：

   发送中断请求，协调者向所有参与者发送 abort 请求；

   中断事务，参与者收到来自协调者的 abort 请求之后，执行事务的中断。

2. DoCommit 阶段
   该阶段进行真正的事务提交，也可以分为以下两种情况。

   A. 执行提交

   发送提交请求。协调者接收到参与者发送的 ACK 响应后，那么它将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求。

   事务提交。参与者接收到 doCommit 请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源。

   响应反馈。事务提交完之后，向协调者发送 ACK 响应。

   完成事务。协调者接收到所有参与者的 ACK 响应之后，完成事务。B. 中断事务
   协调者没有接收到参与者发送的 ACK 响应，可能是因为接受者发送的不是 ACK 响应，也有可能响应超时了，那么就会执行中断事务。

   C.超时提交
   参与者如果没有收到协调者的通知，超时之后会执行 Commit 操作。

**三阶段提交做了哪些改进：**

引入超时机制（协调者和参与者），添加预提交阶段。

然而，参与者precommit后未收到协调者的DoCommit消息也会完成提交，有数据不一致性的风险。

### 2.1.3 TCC 分段提交

TCC 是一个分布式事务的处理模型，将事务过程拆分为 Try、Confirm、Cancel 三个步骤，在保证强一致性的同时，最大限度提高系统的可伸缩性与可用性。

基于业务层面的事务定义，锁粒度完全由业务自己控制，目的是解决复杂业务中，跨表跨库等大颗粒度资源锁定的问题。

![1.png](https://s0.lgstatic.com/i/image/M00/00/D0/Ciqc1F6qgbmAC6GbAAJF3yzrcWs383.png)

Try 阶段：调用 Try 接口，尝试执行业务，完成所有业务检查，预留业务资源。

Confirm 或 Cancel 阶段：两者是互斥的，只能进入其中一个，并且都满足幂等性，允许失败重试。

> Confirm 操作：对业务系统做确认提交，确认执行业务操作，不做其他业务检查，只使用 Try 阶段预留的业务资源。
> Cancel 操作：在业务执行错误，需要回滚的状态下执行业务取消，释放预留资源。



Try阶段失败直接Cancel，Confirm/Cancel阶段失败根据事务日志进行重试。

TCC 的**不足**主要体现在对微服务的侵入性强，TCC 需要对业务系统进行改造，业务逻辑的每个分支都需要实现 try、Confirm、Cancel 三个操作，并且 Confirm、Cancel 必须保证幂等。

### 2.1.4 基于消息补偿的最终一致性

基于消息补偿的一致性（异步事务机制）主要有本地消息表和第三方可靠消息队列等。

本地消息表是一种业务耦合的设计，消息生产方需要额外建一个事务消息表，并记录消息发送状态，消息消费方需要处理这个消息，并完成自己的业务逻辑，另外会有一个异步机制来定期扫描未完成的消息，确保最终一致性。

## 2.2 分布式锁

实现分布式锁目前有三种流行方案，即基于数据库、Redis、ZooKeeper 的方案。